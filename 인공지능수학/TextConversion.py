import streamlit as st
import pandas as pd
import re
import numpy as np
from itertools import zip_longest

# ==============================================================================
# 1. í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ ì •ì˜
# ==============================================================================
st.set_page_config(page_title="TF-IDF ë¶„ì„ê¸°", layout="wide", page_icon="ğŸ§®")

st.markdown("""
<style>
    /* ë‹¨ì–´ ë°°ì§€ ìŠ¤íƒ€ì¼ */
    .word-badge {
        display: inline-block;
        background-color: #f0f2f6;
        color: #31333F;
        border: 1px solid #d6d6d8;
        border-radius: 15px; 
        padding: 5px 12px;
        margin: 4px;
        font-size: 14px;
        font-weight: 500;
        box-shadow: 1px 1px 3px rgba(0,0,0,0.1);
        transition: transform 0.2s;
    }
    .word-badge:hover {
        transform: scale(1.05); 
        background-color: #e0e2e6;
        border-color: #ff4b4b; 
        cursor: pointer;
    }
    /* ê°€ë°© ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .bag-container {
        border: 2px dashed #ff4b4b;
        border-radius: 10px;
        padding: 20px;
        background-color: #fff9f9;
        text-align: center;
        min-height: 200px;
    }
    /* í…Œì´ë¸” ìŠ¤íƒ€ì¼ */
    th {
        text-align: center !important;
        background-color: #e8f4f8 !important;
    }
    td {
        text-align: center !important;
        font-weight: 500;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ§® í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìœ ìš©í•œ ì •ë³´ ì°¾ê¸° (TF-IDF)")

# ==============================================================================
# 2. í—¬í¼ í•¨ìˆ˜ ë° ì´ˆê¸°í™”
# ==============================================================================

# [í•¨ìˆ˜] ì¸ë±ìŠ¤ ìë™ ìƒì„± (0->A, 1->B, ... 26->AA)
def generate_doc_label(n):
    label = ""
    if n < 0: return ""
    while n >= 0:
        label = chr(65 + (n % 26)) + label
        n = n // 26 - 1
    return label

# ì´ˆê¸° ë°ì´í„° ì„¤ì •
default_data = {
    "ë‚´ìš©": [
        "ê²½ì¹˜ê°€ ì¢‹ê³  ì‚¬ì§„ ì°ê¸° ì¢‹ì€ ìº í•‘ì¥",
        "ì‹œì„¤ì´ ê¹¨ë—í•˜ê³  ë·°ê°€ ì¢‹ì€ ìº í•‘ì¥",
        "ì˜¨ìˆ˜ ì˜ ë‚˜ì˜¤ê³  í™”ì¥ì‹¤ì´ ê¹¨ë—í•˜ë‹¤"
    ]
}

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "doc_df" not in st.session_state:
    df = pd.DataFrame(default_data, index=["A", "B", "C"])
    df.index.name = "ë¬¸ì„œëª…"
    st.session_state.doc_df = df

if "wide_token_df" not in st.session_state:
    st.session_state.wide_token_df = None

# ==============================================================================
# 3. [Step 0] í…ìŠ¤íŠ¸ ë°ì´í„° ì…ë ¥ (ìë™ ì¸ë±ìŠ¤ ê´€ë¦¬)
# ==============================================================================
with st.expander("ğŸ“ ë¬¸ì„œ ë°ì´í„° ì…ë ¥ ë° ìˆ˜ì •", expanded=True):
    st.info("ë‚´ìš©ì„ ì…ë ¥í•˜ê±°ë‚˜ í–‰ì„ ì¶”ê°€(+)í•˜ì„¸ìš”. **ë¬¸ì„œëª…(A, B...)ì€ ìë™ìœ¼ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.**")
    
    # 1. ë°ì´í„° ì—ë””í„° (ì¸ë±ìŠ¤ ìˆ˜ì • ë¶ˆê°€)
    input_df = st.data_editor(
        st.session_state.doc_df,
        num_rows="dynamic",       # í–‰ ì¶”ê°€/ì‚­ì œ í—ˆìš©
        use_container_width=True,
        key="input_editor",
        disabled=["_index"]       # ì¸ë±ìŠ¤ ì»¬ëŸ¼ ë¹„í™œì„±í™” (ì½ê¸° ì „ìš©)
    )
    
    # 2. ë³€ê²½ ê°ì§€ ë° ì¸ë±ìŠ¤ ìë™ ì¬ì •ë ¬ ë¡œì§
    if not st.session_state.doc_df.equals(input_df):
        current_rows = len(input_df)
        # í–‰ ê°œìˆ˜ì— ë§ì¶° A, B, C... ì¸ë±ìŠ¤ ì¬ìƒì„±
        new_index = [generate_doc_label(i) for i in range(current_rows)]
        
        input_df.index = new_index
        input_df.index.name = "ë¬¸ì„œëª…"
        
        st.session_state.doc_df = input_df
        st.rerun()
    
    # 3. ë¶„ì„ ì‹œì‘ ë²„íŠ¼
    if st.button("ğŸš€ ë¶„ì„ ì‹œì‘ (í† í°í™”)", type="primary", use_container_width=True):
        st.session_state.doc_df = input_df
        
        # ë¬¸ì„œë³„ í† í° ë¦¬ìŠ¤íŠ¸ ìƒì„±
        token_lists = []
        doc_names = []
        
        for doc_name, row in st.session_state.doc_df.iterrows():
            content = str(row["ë‚´ìš©"])
            # íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ê³µë°± ê¸°ì¤€ ë¶„ë¦¬
            cleaned = re.sub(r'[^\w\s]', '', content)
            tokens = cleaned.split()
            
            token_lists.append(tokens)
            doc_names.append(str(doc_name))
            
        # [í•µì‹¬] ë¦¬ìŠ¤íŠ¸ë“¤ì„ ì„¸ë¡œ(Column)ë¡œ ë°°ì¹˜ (zip_longest)
        # í–‰(ë¬¸ì„œ) ê¸°ë°˜ ë°ì´í„°ë¥¼ ì—´(ë¬¸ì„œ) ê¸°ë°˜ ë°ì´í„°ë¡œ ë³€í™˜
        combined_tokens = list(zip_longest(*token_lists, fillvalue=None))
        wide_df = pd.DataFrame(combined_tokens, columns=doc_names)
        
        st.session_state.wide_token_df = wide_df
        st.rerun()

# ==============================================================================
# 4. ë¶„ì„ í”„ë¡œì„¸ìŠ¤
# ==============================================================================
if st.session_state.wide_token_df is not None:
    st.divider()
    
    # --- [Step 1] ë¶ˆìš©ì–´ ì²˜ë¦¬ ë° ë‹¨ì–´ê°€ë°© ---
    col_edit, col_bag = st.columns([0.5, 0.5], gap="large")
    
    # 1-1. ì™¼ìª½: ë¬¸ì„œë³„ ë‹¨ì–´ í¸ì§‘ (ë¬¸ì„œê°€ ì—´Columnë¡œ ë°°ì¹˜ë¨)
    with col_edit:
        st.subheader("1ï¸âƒ£ ë‹¨ì–´ ë¶„ë¦¬ ë° ë¶ˆìš©ì–´ ì œê±°")
        st.caption("ê° ë¬¸ì„œ(ì—´)ì— í¬í•¨ëœ ë‹¨ì–´ë“¤ì…ë‹ˆë‹¤. ìˆ˜ì •í•˜ê±°ë‚˜ ì§€ìš°ë©´ ê²°ê³¼ì— ë°˜ì˜ë©ë‹ˆë‹¤.")
        
        edited_wide_df = st.data_editor(
            st.session_state.wide_token_df,
            use_container_width=True,
            height=400,
            num_rows="dynamic", # ë‹¨ì–´ ì¶”ê°€/ì‚­ì œ ê°€ëŠ¥
            key="wide_editor"
        )
        
        # ë³€ê²½ ê°ì§€ ë° ë™ê¸°í™”
        if not st.session_state.wide_token_df.equals(edited_wide_df):
            st.session_state.wide_token_df = edited_wide_df
            st.rerun()

    # --- ë°ì´í„° ì¬êµ¬ì„± (Wide DF -> Tokens List & Validated Vocab) ---
    doc_names = edited_wide_df.columns.tolist()
    tokens_by_doc = []
    all_valid_tokens_flat = []

    for doc in doc_names:
        # í•´ë‹¹ ì—´(ë¬¸ì„œ)ì—ì„œ Noneê³¼ ë¹ˆì¹¸ì„ ì œì™¸í•œ ë‹¨ì–´ë“¤ ì¶”ì¶œ
        col_tokens = edited_wide_df[doc].dropna().astype(str).tolist()
        valid_tokens = [t for t in col_tokens if t.strip() != "" and t != "None"]
        
        tokens_by_doc.append(valid_tokens)
        all_valid_tokens_flat.extend(valid_tokens)

    # 1-2. ì˜¤ë¥¸ìª½: ë‹¨ì–´ ê°€ë°© ì‹œê°í™”
    with col_bag:
        st.subheader("2ï¸âƒ£ ë‹¨ì–´ ê°€ë°© (Bag of Words)")
        st.caption("ëª¨ë“  ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ê³ ìœ  ë‹¨ì–´ ëª©ë¡ì…ë‹ˆë‹¤.")
        
        # ì „ì²´ ë‹¨ì–´ì¥(Vocabulary) ìƒì„±
        all_words = sorted(list(set(all_valid_tokens_flat)))
        
        if all_words:
            html_badges = ""
            for word in all_words:
                # ì „ì²´ ë¬¸ì„œì—ì„œì˜ ì´ ë“±ì¥ íšŸìˆ˜
                count = all_valid_tokens_flat.count(word)
                html_badges += f'<span class="word-badge">{word} <small>({count})</small></span>'
            
            st.markdown(f"""
            <div class="bag-container">
                <h4>ğŸ‘œ Vocabulary</h4>
                <div style="margin-top: 15px;">
                    {html_badges}
                </div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.warning("ë‹¨ì–´ ê°€ë°©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            st.stop()

    # --------------------------------------------------------------------------
    # [Step 2] ë‹¨ì–´ë¹ˆë„ (TF)
    # --------------------------------------------------------------------------
    st.divider()
    st.header("3ï¸âƒ£ ë‹¨ì–´ë¹ˆë„ (TF: Term Frequency)")
    st.markdown("ê° ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì˜ ë¹ˆë„ìˆ˜ì…ë‹ˆë‹¤.")
    
    tf_rows = []
    for tokens in tokens_by_doc:
        counts = [tokens.count(word) for word in all_words]
        tf_rows.append(counts)
    
    # indexë¥¼ doc_names(ì¸ë±ìŠ¤ê°’)ë¡œ ì„¤ì •
    df_tf = pd.DataFrame(tf_rows, columns=all_words, index=doc_names)
    st.table(df_tf)

    # --------------------------------------------------------------------------
    # [Step 3] ë¬¸ì„œë¹ˆë„ (DF)
    # --------------------------------------------------------------------------
    st.header("4ï¸âƒ£ ë¬¸ì„œë¹ˆë„ (DF: Document Frequency)")
    st.markdown("ë‹¨ì–´ë³„ë¡œ ê·¸ ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” **ë¬¸ì„œì˜ ê°œìˆ˜**ì…ë‹ˆë‹¤.")
    
    df_counts = []
    for word in all_words:
        count = 0
        for tokens in tokens_by_doc:
            if word in tokens:
                count += 1
        df_counts.append(count)
        
    df_df_table = pd.DataFrame([df_counts], columns=all_words, index=["DF"])
    st.table(df_df_table)

    # --------------------------------------------------------------------------
    # [Step 4] ì—­ë¬¸ì„œë¹ˆë„ (IDF)
    # --------------------------------------------------------------------------
    st.header("5ï¸âƒ£ ì—­ë¬¸ì„œë¹ˆë„ (IDF: Inverse Document Frequency)")
    n_docs = len(doc_names)
    
    st.latex(r"IDF = \frac{\text{ì „ì²´ ë¬¸ì„œì˜ ê°œìˆ˜}(n)}{\text{ë¬¸ì„œë¹ˆë„}(DF)}")
    st.caption(f"í˜„ì¬ ì „ì²´ ë¬¸ì„œì˜ ê°œìˆ˜(n)ëŠ” **{n_docs}**ê°œì…ë‹ˆë‹¤.")

    idf_values = []
    for df_val in df_counts:
        if df_val == 0:
            idf_values.append(0)
        else:
            # ìš”ì²­í•˜ì‹  ê³µì‹: n / DF
            idf_values.append(n_docs / df_val)
    
    # ì†Œìˆ˜ì  í¬ë§·íŒ…
    idf_display = [f"{v:.2f}".rstrip('0').rstrip('.') if v != 0 else "0" for v in idf_values]
    df_idf = pd.DataFrame([idf_display], columns=all_words, index=["IDF"])
    st.table(df_idf)

    # --------------------------------------------------------------------------
    # [Step 5] TF-IDF
    # --------------------------------------------------------------------------
    st.header("6ï¸âƒ£ TF-IDF êµ¬í•˜ê¸°")
    st.markdown("ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìµœì¢… ê°’ì…ë‹ˆë‹¤.")
    st.latex(r"\text{TF-IDF} = \text{TF} \times \text{IDF}")

    tfidf_rows = []
    for i in range(n_docs):
        row_vals = []
        for j in range(len(all_words)):
            tf_val = tf_rows[i][j]
            idf_val = idf_values[j]
            val = tf_val * idf_val
            row_vals.append(val)
        tfidf_rows.append(row_vals)

    df_tfidf = pd.DataFrame(tfidf_rows, columns=all_words, index=doc_names)
    
    # 0ì´ ì•„ë‹Œ ê°’ë§Œ ì†Œìˆ˜ì  í‘œì‹œ
    df_tfidf_display = df_tfidf.applymap(lambda x: f"{x:.2f}".rstrip('0').rstrip('.') if x != 0 else "0")
    
    st.table(df_tfidf_display)
    
    # [ì¸ì‚¬ì´íŠ¸] ê²°ê³¼ í•´ì„
    st.divider()
    st.subheader("ğŸ’¡ ë¶„ì„ ê²°ê³¼ ì¸ì‚¬ì´íŠ¸")
    
    for idx, doc_name in enumerate(doc_names):
        row_series = df_tfidf.iloc[idx]
        max_val = row_series.max()
        
        if max_val > 0:
            # ìµœëŒ€ê°’ì„ ê°€ì§„ ë‹¨ì–´ë“¤ ì°¾ê¸°
            top_words = row_series[row_series == max_val].index.tolist()
            top_words_str = ", ".join([f"'{w}'" for w in top_words])
            st.success(f"ğŸ“„ **ë¬¸ì„œ {doc_name}**ì˜ í•µì‹¬ í‚¤ì›Œë“œ: **{top_words_str}** (ì ìˆ˜: {max_val:.2f})")
        else:
            st.info(f"ğŸ“„ ë¬¸ì„œ {doc_name}ì—ëŠ” íŠ¹ì§•ì ì¸ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

else:
    st.info("ğŸ‘† ìƒë‹¨ì˜ 'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")