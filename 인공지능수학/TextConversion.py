import streamlit as st
import pandas as pd
import re
import numpy as np

# ==============================================================================
# 1. í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼
# ==============================================================================
st.markdown("""
<style>
    /* í…Œì´ë¸” í—¤ë” ìŠ¤íƒ€ì¼ */
    th {
        text-align: center !important;
        background-color: #e8f4f8 !important;
    }
    /* í…Œì´ë¸” ë³¸ë¬¸ ìŠ¤íƒ€ì¼ */
    td {
        text-align: center !important;
        font-weight: 500;
    }
    /* ê°•ì¡° ìŠ¤íƒ€ì¼ */
    .highlight {
        background-color: #fff3cd;
        padding: 2px 5px;
        border-radius: 4px;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ§® í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìœ ìš©í•œ ì •ë³´ ì°¾ê¸°")
st.caption("ë¬¸ì„œ ë‚´ì˜ ë‹¨ì–´ ë¹ˆë„(TF)ì™€ ì—­ë¬¸ì„œ ë¹ˆë„(IDF)ë¥¼ ê³„ì‚°í•˜ì—¬ ê° ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€í•´ë´…ì‹œë‹¤.")

# ==============================================================================
# 2. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ==============================================================================
# ì´ˆê¸° ì˜ˆì‹œ ë°ì´í„°
default_data = [
    {"ë¬¸ì„œëª…": "A", "ë‚´ìš©": "ê²½ì¹˜ê°€ ì¢‹ê³  ì‚¬ì§„ ì°ê¸° ì¢‹ì€ ìº í•‘ì¥"},
    {"ë¬¸ì„œëª…": "B", "ë‚´ìš©": "ì‹œì„¤ì´ ê¹¨ë—í•˜ê³  ë·°ê°€ ì¢‹ì€ ìº í•‘ì¥"},
    {"ë¬¸ì„œëª…": "C", "ë‚´ìš©": "ì˜¨ìˆ˜ ì˜ ë‚˜ì˜¤ê³  í™”ì¥ì‹¤ì´ ê¹¨ë—í•˜ë‹¤"}
]

if "doc_df" not in st.session_state:
    st.session_state.doc_df = pd.DataFrame(default_data)

if "final_tokens" not in st.session_state:
    st.session_state.final_tokens = None

# ==============================================================================
# 3. [Step 0] í…ìŠ¤íŠ¸ ë°ì´í„° ì…ë ¥ (í–‰ ì¶”ê°€ ê°€ëŠ¥)
# ==============================================================================
with st.expander("ğŸ“ ë¬¸ì„œ ë°ì´í„° ì…ë ¥ ë° ìˆ˜ì •", expanded=True):
    st.info("ë¬¸ì„œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”. í–‰ì„ ì¶”ê°€í•˜ê±°ë‚˜ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    input_df = st.data_editor(
        st.session_state.doc_df,
        num_rows="dynamic",
        use_container_width=True,
        key="input_editor"
    )
    
    if st.button("ğŸš€ ë¶„ì„ ì‹œì‘ (í† í°í™”)", type="primary", use_container_width=True):
        st.session_state.doc_df = input_df
        
        # ì „ì²˜ë¦¬ ë° í† í°í™”
        tokenized_data = []
        for idx, row in input_df.iterrows():
            content = str(row["ë‚´ìš©"])
            # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            cleaned = re.sub(r'[^\w\s]', '', content)
            tokens = cleaned.split()
            tokenized_data.append(tokens)
        
        st.session_state.final_tokens = tokenized_data
        st.rerun()

# ==============================================================================
# 4. ë¶„ì„ ê²°ê³¼ í‘œì‹œ
# ==============================================================================
if st.session_state.final_tokens is not None:
    tokens_list = st.session_state.final_tokens
    doc_names = st.session_state.doc_df["ë¬¸ì„œëª…"].tolist()
    
    # ì „ì²´ ë‹¨ì–´ì¥(Vocabulary) ìƒì„± (ì¤‘ë³µ ì œê±° & ì •ë ¬)
    # 2ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¥¼ 1ì°¨ì›ìœ¼ë¡œ í´ê¸°(flatten) -> ì§‘í•©(set) -> ì •ë ¬
    all_words = sorted(list(set([word for sublist in tokens_list for word in sublist])))
    
    if not all_words:
        st.warning("ë¶„ì„í•  ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    st.divider()

    # --------------------------------------------------------------------------
    # [Step 1] ë‹¨ì–´ë¹ˆë„ (TF)
    # --------------------------------------------------------------------------
    st.header("1ï¸âƒ£ ë‹¨ì–´ë¹ˆë„ (TF: Term Frequency)")
    st.markdown("ê° ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì˜ ë¹ˆë„ìˆ˜ì…ë‹ˆë‹¤.")
    
    tf_rows = []
    for tokens in tokens_list:
        # ê° ë‹¨ì–´ê°€ í˜„ì¬ ë¬¸ì„œì— ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ê³„ì‚°
        counts = [tokens.count(word) for word in all_words]
        tf_rows.append(counts)
    
    df_tf = pd.DataFrame(tf_rows, columns=all_words, index=doc_names)
    st.table(df_tf)

    # --------------------------------------------------------------------------
    # [Step 2] ë¬¸ì„œë¹ˆë„ (DF)
    # --------------------------------------------------------------------------
    st.header("2ï¸âƒ£ ë¬¸ì„œë¹ˆë„ (DF: Document Frequency)")
    st.markdown("ë‹¨ì–´ë³„ë¡œ ê·¸ ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” **ë¬¸ì„œì˜ ê°œìˆ˜**ì…ë‹ˆë‹¤.")
    
    # ê° ë‹¨ì–´ê°€ ëª‡ ê°œì˜ ë¬¸ì„œì— ë“±ì¥í–ˆëŠ”ì§€ ê³„ì‚°
    # (ë¬¸ì„œ ë‚´ì— ì—¬ëŸ¬ ë²ˆ ë‚˜ì™€ë„ 1ë²ˆìœ¼ë¡œ ì¹´ìš´íŠ¸ -> set í™œìš©)
    df_counts = []
    for word in all_words:
        count = 0
        for tokens in tokens_list:
            if word in tokens:
                count += 1
        df_counts.append(count)
        
    df_df_table = pd.DataFrame([df_counts], columns=all_words, index=["DF"])
    st.table(df_df_table)

    # --------------------------------------------------------------------------
    # [Step 3] ì—­ë¬¸ì„œë¹ˆë„ (IDF)
    # --------------------------------------------------------------------------
    st.header("3ï¸âƒ£ ì—­ë¬¸ì„œë¹ˆë„ (IDF: Inverse Document Frequency)")
    n_docs = len(tokens_list)
    
    # LaTeX ìˆ˜ì‹ ë Œë”ë§
    st.latex(r"IDF = \frac{\text{ì „ì²´ ë¬¸ì„œì˜ ê°œìˆ˜}(n)}{\text{ë¬¸ì„œë¹ˆë„}(DF)}")
    st.caption(f"í˜„ì¬ ì „ì²´ ë¬¸ì„œì˜ ê°œìˆ˜(n)ëŠ” **{n_docs}**ê°œì…ë‹ˆë‹¤.")

    # IDF ê³„ì‚° (ì´ë¯¸ì§€ ê³µì‹: n / DF)
    # ë¶„ëª¨ê°€ 0ì´ ë  ì¼ì€ ì—†ìŒ (ë‹¨ì–´ì¥ì—ì„œ ê°€ì ¸ì™”ìœ¼ë¯€ë¡œ ìµœì†Œ 1ë²ˆ ë“±ì¥)
    idf_values = [n_docs / df_val for df_val in df_counts]
    
    # ì†Œìˆ˜ì  í¬ë§·íŒ… (ì •ìˆ˜ë©´ ì •ìˆ˜ì²˜ëŸ¼, ì†Œìˆ˜ë©´ 2ìë¦¬ê¹Œì§€)
    idf_display = [f"{v:.2f}".rstrip('0').rstrip('.') for v in idf_values]
    
    df_idf = pd.DataFrame([idf_display], columns=all_words, index=["IDF"])
    st.table(df_idf)

    # --------------------------------------------------------------------------
    # [Step 4] TF-IDF
    # --------------------------------------------------------------------------
    st.header("4ï¸âƒ£ TF-IDF êµ¬í•˜ê¸°")
    st.markdown("ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìµœì¢… ê°’ì…ë‹ˆë‹¤.")
    st.latex(r"\text{TF-IDF} = \text{TF} \times \text{IDF}")

    # TF-IDF ê³„ì‚° (í–‰ë ¬ ê³±ì…ˆì´ ì•„ë‹Œ, ìš”ì†Œë³„ ê³±ì…ˆ)
    tfidf_rows = []
    for i in range(n_docs): # ë¬¸ì„œë³„ ë°˜ë³µ
        row_vals = []
        for j in range(len(all_words)): # ë‹¨ì–´ë³„ ë°˜ë³µ
            tf_val = tf_rows[i][j]
            idf_val = idf_values[j] # ê³„ì‚°ëœ ì‹¤ìˆ˜ê°’ ì‚¬ìš©
            
            val = tf_val * idf_val
            row_vals.append(val)
        tfidf_rows.append(row_vals)

    # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° í¬ë§·íŒ…
    df_tfidf = pd.DataFrame(tfidf_rows, columns=all_words, index=doc_names)
    
    # ì‹œê°ì  í¬ë§·íŒ… (0ì€ ê·¸ëŒ€ë¡œ 0, ë‚˜ë¨¸ì§€ëŠ” ì†Œìˆ˜ì  í‘œì‹œ)
    df_tfidf_display = df_tfidf.applymap(lambda x: f"{x:.2f}".rstrip('0').rstrip('.') if x != 0 else "0")
    
    st.table(df_tfidf_display)
    
    # [ì¸ì‚¬ì´íŠ¸] ê°€ì¥ ë†’ì€ TF-IDF ë‹¨ì–´ ì°¾ê¸°
    st.divider()
    st.subheader("ğŸ’¡ ë¶„ì„ ê²°ê³¼ ì¸ì‚¬ì´íŠ¸")
    
    # ê° ë¬¸ì„œë³„ë¡œ ê°€ì¥ TF-IDFê°€ ë†’ì€ ë‹¨ì–´ ì°¾ê¸°
    for idx, doc_name in enumerate(doc_names):
        # ì‹œë¦¬ì¦ˆë¡œ ë³€í™˜
        row_series = df_tfidf.iloc[idx]
        max_val = row_series.max()
        
        if max_val > 0:
            # ìµœëŒ€ê°’ì„ ê°€ì§„ ë‹¨ì–´ë“¤ ì°¾ê¸°
            top_words = row_series[row_series == max_val].index.tolist()
            top_words_str = ", ".join([f"'{w}'" for w in top_words])
            st.success(f"ğŸ“„ **ë¬¸ì„œ {doc_name}**ì˜ í•µì‹¬ í‚¤ì›Œë“œ: **{top_words_str}** (ì ìˆ˜: {max_val:.2f})")
        else:
            st.info(f"ğŸ“„ ë¬¸ì„œ {doc_name}ì—ëŠ” íŠ¹ì§•ì ì¸ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

else:
    st.info("ğŸ‘† ìƒë‹¨ì˜ 'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")