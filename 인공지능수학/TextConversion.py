import streamlit as st
import pandas as pd
import re
import numpy as np

# ==============================================================================
# 1. í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼
# ==============================================================================
st.set_page_config(page_title="TF-IDF ë¶„ì„ê¸°", layout="wide", page_icon="ğŸ§®")

st.markdown("""
<style>
    .word-badge {
        display: inline-block;
        background-color: #f0f2f6;
        color: #31333F;
        border: 1px solid #d6d6d8;
        border-radius: 15px; 
        padding: 5px 12px;
        margin: 4px;
        font-size: 14px;
        font-weight: 500;
        box-shadow: 1px 1px 3px rgba(0,0,0,0.1);
        transition: transform 0.2s;
    }
    .word-badge:hover {
        transform: scale(1.05); 
        background-color: #e0e2e6;
        border-color: #ff4b4b; 
        cursor: pointer;
    }
    .bag-container {
        border: 2px dashed #ff4b4b;
        border-radius: 10px;
        padding: 20px;
        background-color: #fff9f9;
        text-align: center;
        min-height: 200px;
    }
    th {
        text-align: center !important;
        background-color: #e8f4f8 !important;
    }
    td {
        text-align: center !important;
        font-weight: 500;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ§® í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìœ ìš©í•œ ì •ë³´ ì°¾ê¸° (TF-IDF)")

# ==============================================================================
# 2. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ==============================================================================
# [í•µì‹¬ ë³€ê²½ 1] ë¬¸ì„œëª…ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
default_data = {
    "ë‚´ìš©": [
        "ê²½ì¹˜ê°€ ì¢‹ê³  ì‚¬ì§„ ì°ê¸° ì¢‹ì€ ìº í•‘ì¥",
        "ì‹œì„¤ì´ ê¹¨ë—í•˜ê³  ë·°ê°€ ì¢‹ì€ ìº í•‘ì¥",
        "ì˜¨ìˆ˜ ì˜ ë‚˜ì˜¤ê³  í™”ì¥ì‹¤ì´ ê¹¨ë—í•˜ë‹¤"
    ]
}

if "doc_df" not in st.session_state:
    # ì¸ë±ìŠ¤ë¥¼ A, B, Cë¡œ ì§€ì •í•˜ì—¬ ìƒì„±
    df = pd.DataFrame(default_data, index=["A", "B", "C"])
    df.index.name = "ë¬¸ì„œëª…" # ì¸ë±ìŠ¤ ì»¬ëŸ¼ì˜ ì´ë¦„ ì§€ì •
    st.session_state.doc_df = df

if "token_df" not in st.session_state:
    st.session_state.token_df = None

# ==============================================================================
# 3. [Step 0] í…ìŠ¤íŠ¸ ë°ì´í„° ì…ë ¥ (ì¸ë±ìŠ¤ ì ìš©)
# ==============================================================================
with st.expander("ğŸ“ ë¬¸ì„œ ë°ì´í„° ì…ë ¥ ë° ìˆ˜ì •", expanded=True):
    st.info("ì™¼ìª½ì˜ **'ë¬¸ì„œëª…'** ì—´ë„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í–‰ì„ ì¶”ê°€í•˜ë©´ ìë™ìœ¼ë¡œ ì¸ë±ìŠ¤ê°€ ìƒì„±ë©ë‹ˆë‹¤.")
    
    # [í•µì‹¬ ë³€ê²½ 2] data_editorê°€ ì¸ë±ìŠ¤ë¥¼ ë³´ì—¬ì£¼ë„ë¡ ì„¤ì •
    input_df = st.data_editor(
        st.session_state.doc_df,
        num_rows="dynamic",
        use_container_width=True,
        key="input_editor"
    )
    
    if st.button("ğŸš€ ë¶„ì„ ì‹œì‘ (í† í°í™”)", type="primary", use_container_width=True):
        st.session_state.doc_df = input_df
        
        # 1. ë¬¸ì„œë³„ë¡œ í† í°í™” ìˆ˜í–‰
        all_tokens_data = []
        
        # [í•µì‹¬ ë³€ê²½ 3] iterrows()ì—ì„œ idx(ì¸ë±ìŠ¤=ë¬¸ì„œëª…)ë¥¼ ë°”ë¡œ ì‚¬ìš©
        for doc_name, row in input_df.iterrows():
            content = str(row["ë‚´ìš©"])
            
            # ì „ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
            cleaned = re.sub(r'[^\w\s]', '', content)
            tokens = cleaned.split()
            
            for t in tokens:
                # ë¬¸ì„œëª…(doc_name)ì€ ì¸ë±ìŠ¤ ê°’ì´ë¯€ë¡œ A, B, C ë“±ì´ ë“¤ì–´ì˜´
                all_tokens_data.append({"ë¬¸ì„œëª…": str(doc_name), "ë‹¨ì–´": t})
        
        st.session_state.token_df = pd.DataFrame(all_tokens_data)
        st.rerun()

# ==============================================================================
# 4. ë¶„ì„ í”„ë¡œì„¸ìŠ¤
# ==============================================================================
if st.session_state.token_df is not None:
    st.divider()
    
    # --- [Step 1] ë¶ˆìš©ì–´ ì²˜ë¦¬ ë° ë‹¨ì–´ê°€ë°© ---
    col_edit, col_bag = st.columns([0.5, 0.5], gap="large")
    
    with col_edit:
        st.subheader("1ï¸âƒ£ ë‹¨ì–´ ë¶„ë¦¬ ë° ë¶ˆìš©ì–´ ì œê±°")
        st.caption("í‘œì—ì„œ ë‹¨ì–´ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ì§€ìš°ë©´ ë¶„ì„ ê²°ê³¼ì— ë°˜ì˜ë©ë‹ˆë‹¤.")
        
        edited_token_df = st.data_editor(
            st.session_state.token_df,
            use_container_width=True,
            height=400,
            key="token_editor",
            num_rows="dynamic"
        )
        
        if not st.session_state.token_df.equals(edited_token_df):
            st.session_state.token_df = edited_token_df
            st.rerun()

    with col_bag:
        st.subheader("2ï¸âƒ£ ë‹¨ì–´ ê°€ë°© (Bag of Words)")
        st.caption("ëª¨ë“  ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ê³ ìœ  ë‹¨ì–´ ëª©ë¡ì…ë‹ˆë‹¤.")
        
        valid_df = edited_token_df[edited_token_df["ë‹¨ì–´"].str.strip() != ""]
        valid_df = valid_df.dropna()
        
        all_words = sorted(list(set(valid_df["ë‹¨ì–´"].tolist())))
        
        if all_words:
            html_badges = ""
            total_counts = valid_df["ë‹¨ì–´"].value_counts()
            
            for word in all_words:
                count = total_counts.get(word, 0)
                html_badges += f'<span class="word-badge">{word} <small>({count})</small></span>'
            
            st.markdown(f"""
            <div class="bag-container">
                <h4>ğŸ‘œ Vocabulary</h4>
                <div style="margin-top: 15px;">
                    {html_badges}
                </div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.warning("ë‹¨ì–´ ê°€ë°©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            st.stop()

    # --- ë°ì´í„° ì¤€ë¹„ (ë¬¸ì„œë³„ í† í° ë¦¬ìŠ¤íŠ¸ ì¬êµ¬ì„±) ---
    st.divider()
    
    # ë¬¸ì„œëª… ëª©ë¡ (ì¸ë±ìŠ¤ì—ì„œ ê°€ì ¸ì˜´)
    doc_names = st.session_state.doc_df.index.tolist()
    
    tokens_by_doc = []
    # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ (ì¸ë±ìŠ¤ê°€ ìˆ«ìì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ)
    str_doc_names = [str(d) for d in doc_names]
    
    for doc in str_doc_names:
        doc_tokens = valid_df[valid_df["ë¬¸ì„œëª…"] == doc]["ë‹¨ì–´"].tolist()
        tokens_by_doc.append(doc_tokens)

    # --------------------------------------------------------------------------
    # [Step 2] ë‹¨ì–´ë¹ˆë„ (TF)
    # --------------------------------------------------------------------------
    st.header("3ï¸âƒ£ ë‹¨ì–´ë¹ˆë„ (TF: Term Frequency)")
    
    tf_rows = []
    for tokens in tokens_by_doc:
        counts = [tokens.count(word) for word in all_words]
        tf_rows.append(counts)
    
    # indexë¥¼ doc_names(ì¸ë±ìŠ¤ê°’)ë¡œ ì„¤ì •
    df_tf = pd.DataFrame(tf_rows, columns=all_words, index=doc_names)
    st.table(df_tf)

    # --------------------------------------------------------------------------
    # [Step 3] ë¬¸ì„œë¹ˆë„ (DF)
    # --------------------------------------------------------------------------
    st.header("4ï¸âƒ£ ë¬¸ì„œë¹ˆë„ (DF: Document Frequency)")
    
    df_counts = []
    for word in all_words:
        count = 0
        for tokens in tokens_by_doc:
            if word in tokens:
                count += 1
        df_counts.append(count)
        
    df_df_table = pd.DataFrame([df_counts], columns=all_words, index=["DF"])
    st.table(df_df_table)

    # --------------------------------------------------------------------------
    # [Step 4] ì—­ë¬¸ì„œë¹ˆë„ (IDF)
    # --------------------------------------------------------------------------
    st.header("5ï¸âƒ£ ì—­ë¬¸ì„œë¹ˆë„ (IDF: Inverse Document Frequency)")
    n_docs = len(doc_names)
    
    st.latex(r"IDF = \frac{\text{ì „ì²´ ë¬¸ì„œì˜ ê°œìˆ˜}(n)}{\text{ë¬¸ì„œë¹ˆë„}(DF)}")
    st.caption(f"í˜„ì¬ ì „ì²´ ë¬¸ì„œì˜ ê°œìˆ˜(n)ëŠ” **{n_docs}**ê°œì…ë‹ˆë‹¤.")

    idf_values = []
    for df_val in df_counts:
        if df_val == 0:
            idf_values.append(0)
        else:
            idf_values.append(n_docs / df_val)
    
    idf_display = [f"{v:.2f}".rstrip('0').rstrip('.') if v != 0 else "0" for v in idf_values]
    df_idf = pd.DataFrame([idf_display], columns=all_words, index=["IDF"])
    st.table(df_idf)

    # --------------------------------------------------------------------------
    # [Step 5] TF-IDF
    # --------------------------------------------------------------------------
    st.header("6ï¸âƒ£ TF-IDF êµ¬í•˜ê¸°")
    st.latex(r"\text{TF-IDF} = \text{TF} \times \text{IDF}")

    tfidf_rows = []
    for i in range(n_docs):
        row_vals = []
        for j in range(len(all_words)):
            tf_val = tf_rows[i][j]
            idf_val = idf_values[j]
            val = tf_val * idf_val
            row_vals.append(val)
        tfidf_rows.append(row_vals)

    df_tfidf = pd.DataFrame(tfidf_rows, columns=all_words, index=doc_names)
    df_tfidf_display = df_tfidf.applymap(lambda x: f"{x:.2f}".rstrip('0').rstrip('.') if x != 0 else "0")
    
    st.table(df_tfidf_display)
    
    # [ì¸ì‚¬ì´íŠ¸]
    st.divider()
    st.subheader("ğŸ’¡ ë¶„ì„ ê²°ê³¼ ì¸ì‚¬ì´íŠ¸")
    
    for idx, doc_name in enumerate(doc_names):
        row_series = df_tfidf.iloc[idx]
        max_val = row_series.max()
        
        if max_val > 0:
            top_words = row_series[row_series == max_val].index.tolist()
            top_words_str = ", ".join([f"'{w}'" for w in top_words])
            st.success(f"ğŸ“„ **ë¬¸ì„œ {doc_name}**ì˜ í•µì‹¬ í‚¤ì›Œë“œ: **{top_words_str}** (ì ìˆ˜: {max_val:.2f})")
        else:
            st.info(f"ğŸ“„ ë¬¸ì„œ {doc_name}ì—ëŠ” íŠ¹ì§•ì ì¸ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

else:
    st.info("ğŸ‘† ìƒë‹¨ì˜ 'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")